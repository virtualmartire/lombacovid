{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadData():\n",
    "\n",
    "    first_date = (datetime.date(2020, 2, 24))\n",
    "    last_date = (datetime.date(2022, 8, 4))\n",
    "\n",
    "    for delta in range(1, (last_date - first_date).days):\n",
    "\n",
    "        today = first_date + datetime.timedelta(days=delta)\n",
    "        today_url = 'https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-regioni/dpc-covid19-ita-regioni-'+today.strftime(\"%Y%m%d\")+'.csv'\n",
    "        present = pd.read_csv(today_url)\n",
    "        present.to_csv('date_datasets/'+today.strftime(\"%Y%m%d\")+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "regioni = [\"Abruzzo\", \"Basilicata\", \"Calabria\", \"Campania\", \"Emilia-Romagna\", \"Friuli-Venezia Giulia\", \"Lazio\", \"Liguria\", \"Lombardia\", \"Marche\", \"Molise\", \"P.A. Bolzano\", \"P.A. Trento\", \"Piemonte\", \"Puglia\", \"Sardegna\", \"Sicilia\", \"Toscana\", \"Umbria\", \"Valle d'Aosta\", \"Veneto\"]\n",
    "regioni_no_friuli = [\"Abruzzo\", \"Basilicata\", \"Calabria\", \"Campania\", \"Emilia-Romagna\", \"Lazio\", \"Liguria\", \"Lombardia\", \"Marche\", \"Molise\", \"P.A. Bolzano\", \"P.A. Trento\", \"Piemonte\", \"Puglia\", \"Sardegna\", \"Sicilia\", \"Toscana\", \"Umbria\", \"Valle d'Aosta\", \"Veneto\"]\n",
    "\n",
    "first_date = (datetime.date(2020, 9, 1))   # perch√© all'inizio i dati erano sballati\n",
    "last_date = (datetime.date(2022, 8, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateToRegion(regioni, first_date, last_date):\n",
    "\n",
    "    for regione in regioni_no_friuli:\n",
    "\n",
    "        regione_csv = pd.DataFrame(data={\"perc_story\": [], \"ospedalizzati_story\": []})\n",
    "\n",
    "        for delta in range(1, (last_date - first_date).days):\n",
    "            # open the files\n",
    "            today = first_date + datetime.timedelta(days=delta)\n",
    "            yesterday = today - datetime.timedelta(days=1)\n",
    "            present = pd.read_csv('date_datasets/'+today.strftime(\"%Y%m%d\")+'.csv')\n",
    "            past = pd.read_csv('date_datasets/'+yesterday.strftime(\"%Y%m%d\")+'.csv')\n",
    "            regione_present = present[ present['denominazione_regione'] == regione ]\n",
    "            regione_past = past[ past['denominazione_regione'] == regione ]\n",
    "            # compute the perc\n",
    "            try:\n",
    "                tot_tamponi_present = regione_present['tamponi'].values[0]\n",
    "            except:\n",
    "                print(regione)\n",
    "                print(today)\n",
    "                print(regione_present['tamponi'])\n",
    "            tot_tamponi_past = regione_past['tamponi'].values[0]\n",
    "            tamponi_oggi = tot_tamponi_present - tot_tamponi_past\n",
    "            nuovi_positivi = regione_present['nuovi_positivi'].values[0]\n",
    "            percentuale = np.around(nuovi_positivi / tamponi_oggi * 100, 2)     # <---\n",
    "            # compute the hospitalized\n",
    "            ospedalizzati_attuali = regione_present['totale_ospedalizzati'].values[0]       # <---\n",
    "            # append the row\n",
    "            nuova_riga = pd.DataFrame([[percentuale, ospedalizzati_attuali]],\n",
    "                                        columns=[\"perc_story\", \"ospedalizzati_story\"])\n",
    "            regione_csv = pd.concat([regione_csv, nuova_riga])\n",
    "        \n",
    "        regione_csv.to_csv(f\"region_datasets/{regione}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dateToRegion(regioni_no_friuli, first_date, last_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot every columns\n",
    "\n",
    "# for regione in regioni_no_friuli:\n",
    "#     df = pd.read_csv(f\"region_datasets/{regione}.csv\")\n",
    "#     df.plot(y=['perc_story'], kind=\"line\", figsize=(10, 10), color='red', title=regione)\n",
    "#     df.plot(y=['ospedalizzati_story'], kind=\"line\", figsize=(10, 10), color='orange', title=regione)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all the datasets and window them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessDatasets(regions_array):\n",
    "    \n",
    "    window_length = 14\n",
    "    global_dataset = np.empty([0, window_length])\n",
    "\n",
    "    for regione in regions_array:\n",
    "\n",
    "        dataset = pd.read_csv(f\"region_datasets/{regione}.csv\")\n",
    "        dataset = dataset['ospedalizzati_story'].values\n",
    "\n",
    "        indexer = np.arange(window_length)[None, :] + np.arange(dataset.shape[0]-window_length+1)[:, None]\n",
    "        dataset = dataset[indexer]\n",
    "\n",
    "        global_dataset = np.vstack((global_dataset, dataset))\n",
    "\n",
    "    return global_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "regioni_no_friuli_no_lomba = [\"Abruzzo\", \"Basilicata\", \"Calabria\", \"Campania\", \"Emilia-Romagna\", \"Lazio\", \"Liguria\", \"Marche\", \"Molise\", \"P.A. Bolzano\", \"P.A. Trento\", \"Piemonte\", \"Puglia\", \"Sardegna\", \"Sicilia\", \"Toscana\", \"Umbria\", \"Valle d'Aosta\", \"Veneto\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = preprocessDatasets(regioni_no_friuli_no_lomba)\n",
    "\n",
    "train_input = torch.from_numpy(training_set[:, :-1]).type(torch.FloatTensor)\n",
    "train_target = torch.from_numpy(training_set[:, 1:]).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dtype\n",
    "\n",
    "\n",
    "test_set = preprocessDatasets([\"Lombardia\"])\n",
    "\n",
    "test_input = torch.from_numpy(test_set[:, :-1]).type(torch.FloatTensor)\n",
    "test_target = torch.from_numpy(test_set[:, 1:]).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, hidden_layers=64):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        # lstm1, lstm2, linear are all layers in the network\n",
    "        self.lstm1 = torch.nn.LSTMCell(1, self.hidden_layers)\n",
    "        self.lstm2 = torch.nn.LSTMCell(self.hidden_layers, self.hidden_layers)\n",
    "        self.linear = torch.nn.Linear(self.hidden_layers, 1)\n",
    "        \n",
    "    def forward(self, y, future_preds=0):\n",
    "        outputs, num_samples = [], y.size(0)\n",
    "        h_t = torch.zeros(num_samples, self.hidden_layers, dtype=torch.float32)\n",
    "        c_t = torch.zeros(num_samples, self.hidden_layers, dtype=torch.float32)\n",
    "        h_t2 = torch.zeros(num_samples, self.hidden_layers, dtype=torch.float32)\n",
    "        c_t2 = torch.zeros(num_samples, self.hidden_layers, dtype=torch.float32)\n",
    "        \n",
    "        for time_step in y.split(1, dim=1):\n",
    "            # N, 1\n",
    "            h_t, c_t = self.lstm1(time_step, (h_t, c_t)) # initial hidden and cell states\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2)) # new hidden and cell states\n",
    "            output = self.linear(h_t2) # output from the last FC layer\n",
    "            outputs.append(output)\n",
    "            \n",
    "        for i in range(future_preds):\n",
    "            # this only generates future predictions if we pass in future_preds>0\n",
    "            # mirrors the code above, using last output/prediction as input\n",
    "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs.append(output)\n",
    "        # transform list to tensor    \n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(hidden_layers=64)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimiser = torch.optim.LBFGS(model.parameters(), lr=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, model, optimiser, loss_fn, train_input, train_target, test_input, test_target):\n",
    "\n",
    "    def closure():\n",
    "        optimiser.zero_grad()\n",
    "        out = model(train_input)\n",
    "        loss = loss_fn(out, train_target)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Train\n",
    "        optimiser.step(closure)\n",
    "\n",
    "        # Validate\n",
    "        with torch.no_grad():\n",
    "            future = 0\n",
    "            pred = model(test_input, future_preds=future)\n",
    "            # use all pred samples, but only go to 999\n",
    "            loss_val = loss_fn(pred, test_target)\n",
    "            y = pred.detach().numpy()\n",
    "        # # draw figures\n",
    "        # plt.figure(figsize=(12,6))\n",
    "        # plt.title(f\"Step {i+1}\")\n",
    "        # plt.xlabel(\"x\")\n",
    "        # plt.ylabel(\"y\")\n",
    "        # plt.xticks(fontsize=20)\n",
    "        # plt.yticks(fontsize=20)\n",
    "        # n = train_input.shape[1] # 999\n",
    "        # def draw(yi, colour):\n",
    "        #     plt.plot(np.arange(n), yi[:n], colour, linewidth=2.0)\n",
    "        #     plt.plot(np.arange(n, n+future), yi[n:], colour+\":\", linewidth=2.0)\n",
    "        # draw(y[0], 'r')\n",
    "        # draw(y[1], 'b')\n",
    "        # draw(y[2], 'g')\n",
    "        # plt.savefig(\"predict%d.png\"%i, dpi=200)\n",
    "        # plt.close()\n",
    "\n",
    "        # print the loss\n",
    "        out = model(train_input)\n",
    "        loss_train = loss_fn(out, train_target)\n",
    "        print(f\"Epoch {epoch+1}, Training loss {loss_train.item():.4f}, Validation loss {loss_val.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 741280.0625, Validation loss 10244147.0000\n",
      "Epoch 2, Training loss 544948.9375, Validation loss 8855690.0000\n",
      "Epoch 3, Training loss 1017135.3125, Validation loss 7549261.0000\n",
      "Epoch 4, Training loss 533017.8125, Validation loss 8681516.0000\n",
      "Epoch 5, Training loss 515587.0312, Validation loss 8338853.5000\n",
      "Epoch 6, Training loss 503522.0938, Validation loss 8383300.0000\n",
      "Epoch 7, Training loss 581732.8750, Validation loss 8808748.0000\n",
      "Epoch 8, Training loss 574452.6250, Validation loss 8877343.0000\n",
      "Epoch 9, Training loss 568145.0000, Validation loss 8804914.0000\n",
      "Epoch 10, Training loss 958178.3125, Validation loss 6834364.0000\n",
      "Epoch 11, Training loss 256357696.0000, Validation loss 220921920.0000\n",
      "Epoch 12, Training loss 11091334.0000, Validation loss 31382028.0000\n",
      "Epoch 13, Training loss 1484733.0000, Validation loss 13015797.0000\n",
      "Epoch 14, Training loss 577596.1875, Validation loss 8908980.0000\n",
      "Epoch 15, Training loss 576960.6875, Validation loss 8887211.0000\n",
      "Epoch 16, Training loss 576252.1250, Validation loss 8853346.0000\n",
      "Epoch 17, Training loss 576166.6875, Validation loss 8850570.0000\n",
      "Epoch 18, Training loss 602731.7500, Validation loss 8003575.0000\n",
      "Epoch 19, Training loss 560173.2500, Validation loss 8641286.0000\n",
      "Epoch 20, Training loss 554456.6250, Validation loss 8724199.0000\n",
      "Epoch 21, Training loss 582903.5000, Validation loss 8929275.0000\n",
      "Epoch 22, Training loss 557645.1250, Validation loss 8740728.0000\n",
      "Epoch 23, Training loss 552152.0000, Validation loss 8619829.0000\n",
      "Epoch 24, Training loss 549950.0625, Validation loss 8687303.0000\n",
      "Epoch 25, Training loss 549762.2500, Validation loss 8660011.0000\n",
      "Epoch 26, Training loss 551567.4375, Validation loss 8666717.0000\n",
      "Epoch 27, Training loss 552240.8750, Validation loss 8642229.0000\n",
      "Epoch 28, Training loss 551034.0625, Validation loss 8663579.0000\n",
      "Epoch 29, Training loss 548033.1250, Validation loss 8667737.0000\n",
      "Epoch 30, Training loss 546149.7500, Validation loss 8583821.0000\n",
      "Epoch 31, Training loss 66842316.0000, Validation loss 106376824.0000\n",
      "Epoch 32, Training loss 1066593.3750, Validation loss 7568001.0000\n",
      "Epoch 33, Training loss 696495.6875, Validation loss 9992001.0000\n",
      "Epoch 34, Training loss 651009.8750, Validation loss 9295738.0000\n",
      "Epoch 35, Training loss 646277.0000, Validation loss 9233320.0000\n",
      "Epoch 36, Training loss 640615.8125, Validation loss 9125817.0000\n",
      "Epoch 37, Training loss 623797.8750, Validation loss 8827479.0000\n",
      "Epoch 38, Training loss 621611.1250, Validation loss 8825250.0000\n",
      "Epoch 39, Training loss 621122.3125, Validation loss 8828195.0000\n",
      "Epoch 40, Training loss 620244.4375, Validation loss 8846070.0000\n",
      "Epoch 41, Training loss 617931.2500, Validation loss 8851723.0000\n",
      "Epoch 42, Training loss 617524.5625, Validation loss 8857011.0000\n",
      "Epoch 43, Training loss 617249.8750, Validation loss 8861149.0000\n",
      "Epoch 44, Training loss 617023.3125, Validation loss 8864965.0000\n",
      "Epoch 45, Training loss 615810.9375, Validation loss 8882912.0000\n",
      "Epoch 46, Training loss 614762.6875, Validation loss 8915563.0000\n",
      "Epoch 47, Training loss 614288.2500, Validation loss 8920289.0000\n",
      "Epoch 48, Training loss 1199104.2500, Validation loss 10048740.0000\n",
      "Epoch 49, Training loss 601633.3125, Validation loss 9025369.0000\n",
      "Epoch 50, Training loss 595869.5000, Validation loss 9388388.0000\n",
      "Epoch 51, Training loss 574165.6875, Validation loss 8881551.0000\n",
      "Epoch 52, Training loss 569140.5000, Validation loss 8666974.0000\n",
      "Epoch 53, Training loss 589506.4375, Validation loss 9239302.0000\n",
      "Epoch 54, Training loss 542976.6875, Validation loss 8504950.0000\n",
      "Epoch 55, Training loss 609245.1250, Validation loss 9486570.0000\n",
      "Epoch 56, Training loss 772533.5625, Validation loss 8177812.5000\n",
      "Epoch 57, Training loss 548701.9375, Validation loss 8677785.0000\n",
      "Epoch 58, Training loss 577115.8750, Validation loss 8711153.0000\n",
      "Epoch 59, Training loss 459508.3125, Validation loss 8153066.0000\n",
      "Epoch 60, Training loss 430709.6250, Validation loss 7760126.5000\n",
      "Epoch 61, Training loss 118482116608.0000, Validation loss 97060872192.0000\n",
      "Epoch 62, Training loss 98668355584.0000, Validation loss 88684019712.0000\n",
      "Epoch 63, Training loss 32891744256.0000, Validation loss 33730127872.0000\n",
      "Epoch 64, Training loss 36174114586624.0000, Validation loss 6000610377728.0000\n",
      "Epoch 65, Training loss 1132189081141248.0000, Validation loss 964417592229888.0000\n",
      "Epoch 66, Training loss 47662106782203904.0000, Validation loss 50382732341018624.0000\n",
      "Epoch 67, Training loss 8836279420583936.0000, Validation loss 9179449991888896.0000\n",
      "Epoch 68, Training loss 64468885042102272.0000, Validation loss 29555265544126464.0000\n",
      "Epoch 69, Training loss 348325180600221696.0000, Validation loss 219291924803092480.0000\n",
      "Epoch 70, Training loss 53431175573667840.0000, Validation loss 46121480538292224.0000\n",
      "Epoch 71, Training loss 22708293105352704.0000, Validation loss 16104911884255232.0000\n",
      "Epoch 72, Training loss 9921361568858112.0000, Validation loss 7952572182691840.0000\n",
      "Epoch 73, Training loss 5814242183741440.0000, Validation loss 6695605443231744.0000\n",
      "Epoch 74, Training loss 806168247140352.0000, Validation loss 251659481513984.0000\n",
      "Epoch 75, Training loss 2495243660296192.0000, Validation loss 204799257608192.0000\n",
      "Epoch 76, Training loss 15375811120988160.0000, Validation loss 10254536140652544.0000\n",
      "Epoch 77, Training loss 7266035082199040.0000, Validation loss 6373046956851200.0000\n",
      "Epoch 78, Training loss 19948465232543744.0000, Validation loss 21416549478825984.0000\n",
      "Epoch 79, Training loss 5130226229051392.0000, Validation loss 5646517268381696.0000\n",
      "Epoch 80, Training loss 9461545591373824.0000, Validation loss 5037138181619712.0000\n",
      "Epoch 81, Training loss 51337559405494272.0000, Validation loss 38426415267315712.0000\n",
      "Epoch 82, Training loss 10103978209574912.0000, Validation loss 4667219090866176.0000\n",
      "Epoch 83, Training loss 30262169916407808.0000, Validation loss 4213956965040128.0000\n",
      "Epoch 84, Training loss 15846410687610880.0000, Validation loss 15930242006777856.0000\n",
      "Epoch 85, Training loss 4792149411650076672.0000, Validation loss 4884175786115858432.0000\n",
      "Epoch 86, Training loss 1585939420194799616.0000, Validation loss 1496646294196715520.0000\n",
      "Epoch 87, Training loss 391995858310135808.0000, Validation loss 304855885316882432.0000\n",
      "Epoch 88, Training loss 31158009900040192.0000, Validation loss 11283131415920640.0000\n",
      "Epoch 89, Training loss 8463721609297920.0000, Validation loss 2671547772829696.0000\n",
      "Epoch 90, Training loss 1343906124595724288.0000, Validation loss 760071360121667584.0000\n",
      "Epoch 91, Training loss 62537519853469696.0000, Validation loss 40397968464936960.0000\n",
      "Epoch 92, Training loss 11048528491053056.0000, Validation loss 2126126788902912.0000\n",
      "Epoch 93, Training loss 342165682101682176.0000, Validation loss 279196118603005952.0000\n",
      "Epoch 94, Training loss 20532695752484323328.0000, Validation loss 19613563405291487232.0000\n",
      "Epoch 95, Training loss 432546018941206528.0000, Validation loss 219536600500011008.0000\n",
      "Epoch 96, Training loss 282168132892622848.0000, Validation loss 241232610615885824.0000\n",
      "Epoch 97, Training loss 264818183003701248.0000, Validation loss 216964842802642944.0000\n",
      "Epoch 98, Training loss 1442034101400895488.0000, Validation loss 1378615782539264000.0000\n",
      "Epoch 99, Training loss 753276721859395584.0000, Validation loss 491997849605636096.0000\n",
      "Epoch 100, Training loss 107388510410899456.0000, Validation loss 69399498907975680.0000\n"
     ]
    }
   ],
   "source": [
    "training_loop(100, model, optimiser, criterion, train_input, train_target, test_input, test_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('lombacovid')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "206f32289a1c6ae08017adf82f74883987f76632b2f8a15c374d45f2d79d42ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
