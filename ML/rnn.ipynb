{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadData():\n",
    "\n",
    "    first_date = (datetime.date(2020, 2, 24))\n",
    "    last_date = (datetime.date(2022, 8, 4))\n",
    "\n",
    "    for delta in range(1, (last_date - first_date).days):\n",
    "\n",
    "        today = first_date + datetime.timedelta(days=delta)\n",
    "        today_url = 'https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-regioni/dpc-covid19-ita-regioni-'+today.strftime(\"%Y%m%d\")+'.csv'\n",
    "        present = pd.read_csv(today_url)\n",
    "        present.to_csv('date_datasets/'+today.strftime(\"%Y%m%d\")+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regioni = [\"Abruzzo\", \"Basilicata\", \"Calabria\", \"Campania\", \"Emilia-Romagna\", \"Friuli-Venezia Giulia\", \"Lazio\", \"Liguria\", \"Lombardia\", \"Marche\", \"Molise\", \"P.A. Bolzano\", \"P.A. Trento\", \"Piemonte\", \"Puglia\", \"Sardegna\", \"Sicilia\", \"Toscana\", \"Umbria\", \"Valle d'Aosta\", \"Veneto\"]\n",
    "regioni_no_friuli = [\"Abruzzo\", \"Basilicata\", \"Calabria\", \"Campania\", \"Emilia-Romagna\", \"Lazio\", \"Liguria\", \"Lombardia\", \"Marche\", \"Molise\", \"P.A. Bolzano\", \"P.A. Trento\", \"Piemonte\", \"Puglia\", \"Sardegna\", \"Sicilia\", \"Toscana\", \"Umbria\", \"Valle d'Aosta\", \"Veneto\"]\n",
    "\n",
    "first_date = (datetime.date(2020, 9, 1))   # perch√© all'inizio i dati erano sballati\n",
    "last_date = (datetime.date(2022, 8, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateToRegion(regioni, first_date, last_date):\n",
    "\n",
    "    for regione in regioni_no_friuli:\n",
    "\n",
    "        regione_csv = pd.DataFrame(data={\"perc_story\": [], \"ospedalizzati_story\": []})\n",
    "\n",
    "        for delta in range(1, (last_date - first_date).days):\n",
    "            # open the files\n",
    "            today = first_date + datetime.timedelta(days=delta)\n",
    "            yesterday = today - datetime.timedelta(days=1)\n",
    "            present = pd.read_csv('date_datasets/'+today.strftime(\"%Y%m%d\")+'.csv')\n",
    "            past = pd.read_csv('date_datasets/'+yesterday.strftime(\"%Y%m%d\")+'.csv')\n",
    "            regione_present = present[ present['denominazione_regione'] == regione ]\n",
    "            regione_past = past[ past['denominazione_regione'] == regione ]\n",
    "            # compute the perc\n",
    "            try:\n",
    "                tot_tamponi_present = regione_present['tamponi'].values[0]\n",
    "            except:\n",
    "                print(regione)\n",
    "                print(today)\n",
    "                print(regione_present['tamponi'])\n",
    "            tot_tamponi_past = regione_past['tamponi'].values[0]\n",
    "            tamponi_oggi = tot_tamponi_present - tot_tamponi_past\n",
    "            nuovi_positivi = regione_present['nuovi_positivi'].values[0]\n",
    "            percentuale = np.around(nuovi_positivi / tamponi_oggi * 100, 2)     # <---\n",
    "            # compute the hospitalized\n",
    "            ospedalizzati_attuali = regione_present['totale_ospedalizzati'].values[0]       # <---\n",
    "            # append the row\n",
    "            nuova_riga = pd.DataFrame([[percentuale, ospedalizzati_attuali]],\n",
    "                                        columns=[\"perc_story\", \"ospedalizzati_story\"])\n",
    "            regione_csv = pd.concat([regione_csv, nuova_riga])\n",
    "        \n",
    "        regione_csv.to_csv(f\"region_datasets/{regione}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dateToRegion(regioni_no_friuli, first_date, last_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessDatasets(regions_array, window_length, perc_shift=14):\n",
    "\n",
    "    global_dataset = np.empty([0, window_length+1, 3])\n",
    "\n",
    "    for regione in regions_array:\n",
    "\n",
    "        dataset = pd.read_csv(f\"region_datasets/{regione}.csv\").replace([np.inf, -np.inf], np.nan).interpolate()\n",
    "\n",
    "        # mean and shift the perc\n",
    "        perc = dataset['perc_story']\n",
    "        perc = perc.rolling(window=7, min_periods=1).mean()\n",
    "        perc = perc.shift(perc_shift).dropna()\n",
    "        perc = perc.values\n",
    "\n",
    "        # drop the first perc_shift ospedalizzati\n",
    "        ospedalizzati = dataset['ospedalizzati_story'][perc_shift:].values\n",
    "\n",
    "        dataset = np.column_stack((np.arange(len(perc)), ospedalizzati, perc))\n",
    "\n",
    "        # create the windows\n",
    "        indexer = np.arange(window_length+1)[None, :] + np.arange(dataset.shape[0]-window_length)[:, None]\n",
    "        dataset = dataset[indexer]\n",
    "\n",
    "        global_dataset = np.vstack((global_dataset, dataset))\n",
    "\n",
    "    return global_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regioni_no_friuli_no_lomba = [\"Abruzzo\", \"Basilicata\", \"Calabria\", \"Campania\", \"Emilia-Romagna\", \"Lazio\", \"Liguria\", \"Marche\", \"Molise\", \"P.A. Bolzano\", \"P.A. Trento\", \"Piemonte\", \"Puglia\", \"Sardegna\", \"Sicilia\", \"Toscana\", \"Umbria\", \"Valle d'Aosta\", \"Veneto\"]\n",
    "# regioni_pop = [[\"Abruzzo\", 1_312_000],\n",
    "#                                 [\"Basilicata\", 562_869],\n",
    "#                                 [\"Calabria\", 1_947_000],\n",
    "#                                 [\"Campania\", 5_576_303],\n",
    "#                                 [\"Emilia-Romagna\", 4_459_000],\n",
    "#                                 [\"Lazio\", 5_879_000],\n",
    "#                                 [\"Liguria\", 1_551_000],\n",
    "#                                 [\"Marche\", 1_525_000],\n",
    "#                                 [\"Molise\", 305_617],\n",
    "#                                 [\"P.A. Bolzano\", 535_829],\n",
    "#                                 [\"P.A. Trento\", 542_150],\n",
    "#                                 [\"Piemonte\", 4_245_059],\n",
    "#                                 [\"Puglia\", 3_900_822],\n",
    "#                                 [\"Sardegna\", 1_573_089],\n",
    "#                                 [\"Sicilia\", 4_789_826],\n",
    "#                                 [\"Toscana\", 3_678_941],\n",
    "#                                 [\"Umbria\", 858_478],\n",
    "#                                 [\"Valle d'Aosta\", 123_102],\n",
    "#                                 [\"Veneto\", 4_848_069]]\n",
    "\n",
    "window_length = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = preprocessDatasets(regioni_no_friuli_no_lomba, window_length)\n",
    "\n",
    "batch_size, timesteps, features = training_set.shape\n",
    "training_set = training_set.reshape(batch_size * timesteps, features)\n",
    "training_set = scaler.fit_transform(training_set)                       # <---\n",
    "training_set = training_set.reshape(batch_size, timesteps, features)\n",
    "\n",
    "epsilon = np.finfo(float).eps\n",
    "training_set = -np.log(training_set + epsilon)\n",
    "\n",
    "print(training_set.max())\n",
    "\n",
    "train_input = torch.from_numpy(training_set[:, :-1, :]).type(torch.FloatTensor)\n",
    "train_target = torch.from_numpy(training_set[:, 1:, 1]).type(torch.FloatTensor)        # dim=1 has hospitalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = preprocessDatasets([\"Lombardia\"], window_length)\n",
    "#test_set = preprocessDatasets([[\"Lombardia\", 9_961_801]], window_length)\n",
    "\n",
    "batch_size, timesteps, features = test_set.shape\n",
    "test_set = test_set.reshape(batch_size * timesteps, features)\n",
    "#test_set = scaler.transform(test_set)                              # <---\n",
    "test_set = scaler.fit_transform(test_set)\n",
    "test_set = test_set.reshape(batch_size, timesteps, features)\n",
    "\n",
    "test_set = -np.log(test_set + epsilon)\n",
    "\n",
    "print(test_set.max())\n",
    "\n",
    "test_input = torch.from_numpy(test_set[:, :-1, :]).type(torch.FloatTensor)\n",
    "test_target = torch.from_numpy(test_set[:, 1:, 1]).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_neurons):\n",
    "\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "\n",
    "        self.lstm1 = torch.nn.LSTMCell(3, self.hidden_neurons[0])\n",
    "        self.lstm2 = torch.nn.LSTMCell(self.hidden_neurons[0], self.hidden_neurons[1])\n",
    "        self.lstm3 = torch.nn.LSTMCell(self.hidden_neurons[1], self.hidden_neurons[2])\n",
    "        \n",
    "        self.linear = torch.nn.Linear(self.hidden_neurons[2], 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, y, predict_perc=None):\n",
    "        \"\"\"y.shape = (batch_size, timesteps, features); features = (day_intindex, ospedalizzati, perc))\"\"\"\n",
    "\n",
    "        # # Normalize the input feature-wise\n",
    "\n",
    "        input_batch = y.clone()\n",
    "\n",
    "        # mins = []\n",
    "        # maxs = []\n",
    "        # for i in range(input_batch.size(2)):\n",
    "        #     if i == 1 and predict_perc is not None:\n",
    "        #         # consider also the future percentage for the normalization\n",
    "        #         mins.append( torch.min(input_batch[:, :, i].min(), predict_perc.min()) )\n",
    "        #         maxs.append( torch.max(input_batch[:, :, i].max(), predict_perc.max()) )\n",
    "        #         input_batch[:, :, i] = (input_batch[:, :, i] - mins[i]) / (maxs[i] - mins[i])\n",
    "        #     else:\n",
    "        #         mins.append(input_batch[:, :, i].min())\n",
    "        #         maxs.append(input_batch[:, :, i].max())\n",
    "        #         input_batch[:, :, i] = (input_batch[:, :, i] - mins[i]) / (maxs[i] - mins[i])\n",
    "\n",
    "        # Initialize the cells\n",
    "\n",
    "        outputs, num_samples = [], input_batch.size(0)\n",
    "        h_t = torch.zeros(num_samples, self.hidden_neurons[0], dtype=torch.float32)\n",
    "        c_t = torch.zeros(num_samples, self.hidden_neurons[0], dtype=torch.float32)\n",
    "        h_t2 = torch.zeros(num_samples, self.hidden_neurons[1], dtype=torch.float32)\n",
    "        c_t2 = torch.zeros(num_samples, self.hidden_neurons[1], dtype=torch.float32)\n",
    "        h_t3 = torch.zeros(num_samples, self.hidden_neurons[2], dtype=torch.float32)\n",
    "        c_t3 = torch.zeros(num_samples, self.hidden_neurons[2], dtype=torch.float32)\n",
    "\n",
    "        # Process\n",
    "\n",
    "        known_timesteps = input_batch.size(1)\n",
    "        for i in range(known_timesteps):\n",
    "            h_t, c_t = self.lstm1(input_batch[:, i, :], (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            h_t3, c_t3 = self.lstm3(h_t2, (h_t3, c_t3))\n",
    "            output = self.linear(h_t3)\n",
    "            output = self.relu(output)\n",
    "            outputs.append(output)\n",
    "\n",
    "        # and in case of forecasting...\n",
    "        if predict_perc is not None:\n",
    "            #predict_perc = (predict_perc - mins[1]) / (maxs[1] - mins[1])\n",
    "            last_index = input_batch[0, -1, 0]\n",
    "            predict_days = torch.arange(last_index+1, last_index+1+predict_perc.size(0))\n",
    "            for future_perc, day_intindex in zip(predict_perc, predict_days):\n",
    "                output = torch.column_stack((day_intindex.unsqueeze(0), output, future_perc.unsqueeze(0)))\n",
    "                h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "                h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "                h_t3, c_t3 = self.lstm3(h_t2, (h_t3, c_t3))\n",
    "                output = self.linear(h_t3)\n",
    "                output = self.relu(output)\n",
    "                outputs.append(output)\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "\n",
    "        # # Unnormalize the output\n",
    "\n",
    "        # outputs = (outputs * (maxs[0] - mins[0])) + mins[0]\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(hidden_neurons=[32, 32, 32])\n",
    "\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "# reg_variance = 1e2\n",
    "# regularisation_function = lambda pred, target: torch.mean( torch.exp(-(pred - target)**2/reg_variance) )\n",
    "\n",
    "optimiser = lambda lr: torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, model, optimiser, loss_function,\n",
    "                    train_input, train_target, test_input, test_target):\n",
    "\n",
    "    def closure():\n",
    "        optimiser.zero_grad()\n",
    "        out = model(train_input)\n",
    "        loss = loss_function(out, train_target)\n",
    "        #loss = loss_function(torch.exp(out), torch.exp(train_target))   # to emphasize the distances\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Train\n",
    "        optimiser.step(closure)\n",
    "\n",
    "        # Validate\n",
    "        with torch.no_grad():\n",
    "            pred = model(test_input)\n",
    "            loss_val = loss_function(pred, test_target)\n",
    "\n",
    "        # print the loss\n",
    "        out = model(train_input)\n",
    "        loss_train = loss_function(out, train_target)\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Training loss {loss_train.item():.2e}, Validation loss {loss_val.item():.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"lstm_weights.pt\"))\n",
    "\n",
    "training_loop(500, model, optimiser(1e-2), loss_function,\n",
    "                train_input, train_target, test_input, test_target)\n",
    "\n",
    "torch.save(model.state_dict(), \"lstm_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 50, 10):\n",
    "\n",
    "    plt.plot(test_target[i], label=\"target\")\n",
    "    with torch.no_grad():\n",
    "        pred = model(test_input[i:i+1])\n",
    "    plt.plot(pred[0], label=\"prediction\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(test_input[i, :, 1], label=\"input\")\n",
    "    with torch.no_grad():\n",
    "        pred = model(test_input[i:i+1])\n",
    "    plt.plot(pred[0], label=\"prediction\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0, 300, 50):\n",
    "\n",
    "    t = train_input[x:1+x]\n",
    "\n",
    "    days = t[0, :, 0].flip(0)\n",
    "    ospedalizzati_1 = t[0, :14, 1]\n",
    "    ospedalizzati_2 = t[0, 14:, 1]\n",
    "    perc_1 = t[0, :14, 2]\n",
    "    perc_2 = t[0, 14:, 2]\n",
    "\n",
    "    output = model(t[:, :14, :], predict_perc=perc_2)\n",
    "\n",
    "    plt.plot(days[:14], ospedalizzati_1, label=\"past\")\n",
    "    plt.plot(days[14:], ospedalizzati_2, label=\"target\")\n",
    "    plt.plot(days, output[0].detach(), label=\"prediction\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    #plt.plot(np.concatenate([perc_1, perc_2]), label=\"perc\", color=\"red\")\n",
    "    #plt.legend()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1, 60, 5):\n",
    "\n",
    "    t = test_input[x:1+x]\n",
    "\n",
    "    days = t[0, :, 0].flip(0)\n",
    "    ospedalizzati_1 = t[0, :14, 1]\n",
    "    ospedalizzati_2 = t[0, 14:, 1]\n",
    "    perc_1 = t[0, :14, 2]\n",
    "    perc_2 = t[0, 14:, 2]\n",
    "\n",
    "    output = model(t[:, :14, :], predict_perc=perc_2)\n",
    "\n",
    "    plt.plot(days[:14], ospedalizzati_1, label=\"past\")\n",
    "    plt.plot(days[14:], ospedalizzati_2, label=\"target\")\n",
    "    plt.plot(days, output[0].detach(), label=\"prediction\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    #plt.plot(np.concatenate([perc_1, perc_2]), label=\"perc\", color=\"red\")\n",
    "    #plt.legend()\n",
    "    #plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('lombacovid')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "206f32289a1c6ae08017adf82f74883987f76632b2f8a15c374d45f2d79d42ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
